{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms, models\n",
    "from PIL import Image\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data direction & split size setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = './data'\n",
    "model_dir = 'animal_model.pth'\n",
    "test_dir = './test_data'    # ./data   ..train시킨 자료 내에서 test할 경우 변경\n",
    "\n",
    "# hyper parameter\n",
    "valid_size = 0.3\n",
    "learning_rate = 0.0005\n",
    "avgpool = 512   # FCL 설정 시 feature 수 조정\n",
    "dropout = 0.2\n",
    "optimizer_type = 'NAdam' # Adam, AdaGrad, NAdam\n",
    "epochs = 15         # 에폭을 설정한다.\n",
    "print_every = 5     # 출력 간격을 설정한다.\n",
    "criterion = nn.CrossEntropyLoss() # 손실함수를 Cross entropy loss 함수로 지정한다.\n",
    "t_transforms = transforms.Compose([ transforms.RandomResizedCrop(224),\n",
    "                                    transforms.Resize(224),\n",
    "                                    transforms.ToTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_split_train_test(data_dir, transform, valid_size):\n",
    "    train_data = datasets.ImageFolder(data_dir, transform=t_transforms)\n",
    "    test_data = datasets.ImageFolder(data_dir, transform=t_transforms)\n",
    "\n",
    "    num_train = len(train_data)\n",
    "    indicies = list(range(num_train))\n",
    "\n",
    "    np.random.shuffle(indicies)\n",
    "\n",
    "    split = int(np.floor(num_train * valid_size))\n",
    "\n",
    "    train_idx, test_idx = indicies[split:], indicies[:split]\n",
    "\n",
    "    train_sampler = SubsetRandomSampler(train_idx)\n",
    "    test_sampler = SubsetRandomSampler(test_idx)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(train_data, sampler=train_sampler, batch_size=16)\n",
    "    test_loader = torch.utils.data.DataLoader(test_data, sampler=test_sampler, batch_size=16)\n",
    "\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, test_loader = load_split_train_test(data_dir, t_transforms, valid_size)\n",
    "classes = train_loader.dataset.classes\n",
    "output_len = len(train_loader.dataset.classes)\n",
    "\n",
    "# 학습 loader와 테스트 loader의 class들을 출력하여 확인한다. \n",
    "print(train_loader.dataset.classes) \n",
    "print(test_loader.dataset.classes) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute device를 정하고 확인한다.\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## resnet setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.resnet50(pretrained=True)\n",
    "\n",
    "# 모든 신경망 구축 : 전이학습을 위해 모델의 가중치를 freeze 한다.\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "# 뉴런들을 연결하여 신경망을 생성한다.\n",
    "model.fc = nn.Sequential(nn.Linear(2048,avgpool), nn.ReLU(),nn.Dropout(valid_size),nn.Linear(avgpool,output_len),nn.LogSoftmax(dim=1))\n",
    "# q: explain the above code\n",
    "# a: 2048개의 입력을 받아 512개의 출력을 내고, ReLU 함수를 거쳐 0.2의 확률로 Dropout을 적용한다.\n",
    "# 512개의 입력을 받아 2개의 출력을 내고, LogSoftmax 함수를 거쳐 1차원으로 변환한다.\n",
    "# 1차원으로 변환된 출력을 갖는 신경망을 생성한다.\n",
    "\n",
    "optimizer = optim.Adam(model.fc.parameters(), lr=learning_rate)\n",
    "if optimizer_type=='Adam':\n",
    "    optimizer = optim.Adam(model.fc.parameters(), lr=learning_rate)\n",
    "elif optimizer_type=='AdaGrad':\n",
    "    optimizer = optim.Adagrad(model.fc.parameters(), lr=learning_rate)\n",
    "elif optimizer_type=='NAdam':\n",
    "    optimizer = optim.NAdam(model.fc.parameters(), lr=learning_rate)    \n",
    "# 신경망을 compute device로 보낸다.\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## epoch, sequence setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 손실 변수들을 초기화 한다.\n",
    "running_loss = 0\n",
    "train_losses, test_losses = [],[]\n",
    "# 현재의 학습 단계를 표현하는 steps 변수를 0으로 초기화 한다.\n",
    "steps = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 설정한 회수만큼 학습 후 테스트 및 평가해 본다.\n",
    "for epoch in range(epochs):\n",
    "    epoch +=1\n",
    "    for inputs, labels in train_loader:\n",
    "        steps +=1\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logps = model.forward(inputs)\n",
    "        loss = criterion(logps, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        if steps % print_every == 0:\n",
    "            test_loss = 0\n",
    "            accuracy = 0\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                for inputs, labels in test_loader:\n",
    "                    inputs, labels = inputs.to(device), labels.to(device)\n",
    "                    logps = model.forward(inputs)\n",
    "                    batch_loss = criterion(logps, labels)\n",
    "                    test_loss += batch_loss.item()\n",
    "                    ps = torch.exp(logps)\n",
    "                    top_p, top_class = ps.topk(1, dim=1)\n",
    "                    equals = top_class == labels.view(*top_class.shape)\n",
    "                    accuracy += torch.mean(equals.type(torch.FloatTensor)).item()\n",
    "                train_losses.append(running_loss/len(train_loader))\n",
    "                test_losses.append(test_loss/len(test_loader))\n",
    "                print(\"Epoch {}/{}: \".format(epoch, epochs), \"Train loss: {:.3f}..\".format(running_loss/print_every),\n",
    "                          \"Test loss: {:.3f}..\".format(test_loss/len(test_loader)), \"Test accuracy: {:.3f}\".format(accuracy/len(test_loader)))\n",
    "                running_loss = 0\n",
    "                model.train()\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plot chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "\n",
    "plt.plot(train_losses, label='training loss')\n",
    "plt.plot(test_losses, label='validation loss')\n",
    "plt.legend(frameon = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_image(device, model, trans, image):\n",
    "    image_tensor = trans(image).float()\n",
    "    input = image_tensor.unsqueeze_(0)\n",
    "    input = input.to(device)\n",
    "    output = model(input)\n",
    "    \n",
    "    scores = torch.nn.functional.softmax(output.data, dim=1)\n",
    "    if device == torch.device('cuda'):\n",
    "        scores = scores.cpu().numpy()\n",
    "    else: #elif device ==  torch.device('cpu')\n",
    "        scores = scores.numpy()\n",
    "\n",
    "    index = scores.argmax()\n",
    "    score = scores[0][index]*100.0\n",
    "    print(f'{classes[index]}: {score:.2f}%')\n",
    "    \n",
    "    return index, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_images(dir, trans, num):\n",
    "    data = datasets.ImageFolder(dir, trans)\n",
    "    indicies = list(range(len(data)))\n",
    "    \n",
    "    np.random.shuffle(indicies)\n",
    "    idx = indicies[:num]\n",
    "\n",
    "    sampler = SubsetRandomSampler(idx)\n",
    "    loader = torch.utils.data.DataLoader(data, sampler=sampler, batch_size=num)\n",
    "\n",
    "    images, labels = next(iter(loader))\n",
    "\n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 저장한 모델을 불러온다.\n",
    "device = torch.device('cuda'if torch.cuda.is_available() else 'cpu')\n",
    "model = torch.load(model_dir)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "to_pil = transforms.ToPILImage()\n",
    "images, labels = get_random_images(test_dir, t_transforms, 5)\n",
    "fig = plt.figure(figsize=(20,20))\n",
    "classes = train_loader.dataset.classes\n",
    "\n",
    "for ii in range(len(images)):\n",
    "    image = to_pil(images[ii])\n",
    "    index, probability = predict_image(device, model, t_transforms, image)\n",
    "    sub = fig.add_subplot(1, len(images), ii+1)\n",
    "    res = labels[ii].item() == index\n",
    "    sub.set_title(classes[index]+':'+str(res))\n",
    "    plt.axis('off')\n",
    "    plt.imshow(image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# gradio test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradio_predict(image):\n",
    "    # 저장한 모델을 불러온다.\n",
    "    device = torch.device('cuda'if torch.cuda.is_available() else 'cpu')\n",
    "    model = torch.load(model_dir)\n",
    "    model.eval()\n",
    "\n",
    "    #image = transforms.ToPILImage(image)\n",
    "    index, probability = predict_image(device, model, t_transforms, image)\n",
    "\n",
    "    return f'{classes[index]}: {probability:.2f}%'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr \n",
    "\n",
    "# Create Gradio interface\n",
    "iface = gr.Interface(\n",
    "    fn=gradio_predict,\n",
    "    inputs=gr.Image(type=\"pil\"),\n",
    "    outputs=\"text\",\n",
    ")\n",
    "\n",
    "# Launch the interface\n",
    "iface.launch(share=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
